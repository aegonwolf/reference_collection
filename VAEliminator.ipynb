{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VAEliminator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4qdeR6C4Qid"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision import transforms\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import os\n",
        "from datetime import datetime,timedelta\n",
        "from datetime import date\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import albumentations as A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG9SrHiKK5F7"
      },
      "source": [
        "def show_tensor_images(image_tensor, num_images=25, size=(3, 256, 256)):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "    fig, ax = plt.subplots(figsize = (15, 5))\n",
        "    im = ax.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br0u2Z8O4RZr"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        " \n",
        "##### Declare the model architecture\n",
        "d = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMNYc7MI7gyF"
      },
      "source": [
        "#play around with this architecture is pretty much arbitrary \n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,padding=1,stride=1),\n",
        "            nn.ReLU(),\n",
        "        nn.MaxPool2d(3, stride=2, padding = 1),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,padding=1,stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.MaxPool2d(3, stride=2, padding = 1),\n",
        "       \n",
        "        )\n",
        "        \n",
        "        self.fc1 = nn.Sequential(\n",
        "            ### Reduce the number of channels to 1 without changing the width and dimensions of the images\n",
        "            nn.Linear(64*64*32,128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128,64),\n",
        "            \n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,32)\n",
        "            \n",
        "        )\n",
        " \n",
        "        self.encoder = nn.Sequential(\n",
        "            \n",
        "            nn.Linear(32, d ** 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d ** 2, d * 2)\n",
        "        )\n",
        "        \n",
        " \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(d, d ** 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d ** 2, 32)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(32,64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Linear(64,256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Linear(256,256*256*1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256*256*1)\n",
        "            \n",
        "        )\n",
        "        self.tconv1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=1,out_channels=1,kernel_size=3,stride=1,padding=1),\n",
        "            #nn.ReLU(),\n",
        "            #nn.BatchNorm2d(1),\n",
        "            #nn.ConvTranspose2d(in_channels=1,out_channels=1,kernel_size=3,stride=1,padding=1),\n",
        "            nn.Sigmoid()\n",
        "        \n",
        "        )\n",
        " \n",
        "    def reparameterise(self, mu, logvar):\n",
        "        if self.training:\n",
        "            ## Using log variance to ensure that we get a positive std dev\n",
        "            ## Converting to std dev in the real space\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            ### Create error term which has the same shape as std dev sampled from a N(0,1) distribution\n",
        "            eps = std.data.new(std.size()).normal_()\n",
        "            #eps = torch.zeros(std.size())\n",
        "            ### Add the mean and the std_dev \n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        " \n",
        "    def forward(self, x):\n",
        "        \n",
        "        conv1_output = self.conv1(x)\n",
        "        #conv1_output = self.resnet_adder(conv1_output + x) \n",
        "        fc1_output = self.fc1(conv1_output.view(-1,64*64*32))\n",
        "        \n",
        "        ### Convert Encoded vector into shape (N,2,d)\n",
        "        mu_logvar = self.encoder(fc1_output).view(-1, 2, d)\n",
        "        ### First vector for each image is mean of the latent distribution\n",
        "        mu = mu_logvar[:, 0, :]\n",
        "        ### Second vector for each image is log-variance of the latent distribution\n",
        "        logvar = mu_logvar[:, 1, :]\n",
        "        ### Create variable Z = mu + error * Std_dev\n",
        "        z = self.reparameterise(mu, logvar)\n",
        "        ### Get decoder output\n",
        "        decoder_output = self.decoder(z)\n",
        "        \n",
        "        fc2_output = self.fc2(decoder_output)\n",
        "        tconv1_output = self.tconv1(fc2_output.view(fc2_output.size(0),1,256,256))\n",
        "        ## Resize Decoder Output to Pass it to TransposedConv2d layer to recontruct 3 channeled image\n",
        "        ## Return Reconstructed Output and mean and log-variance\n",
        "        return tconv1_output, mu, logvar,z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF_FGOB1BlDh"
      },
      "source": [
        "model = VAE()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_OnE9J3Bu7w"
      },
      "source": [
        "learning_rate = 3e-3\n",
        "recon_criterion = nn.L1Loss()\n",
        "lambda_recon = 900\n",
        "#MSE = nn.MSELoss(reduction='sum')\n",
        " \n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=learning_rate\n",
        "    )\n",
        "### Loss function\n",
        " \n",
        "#Reconstruction + KL divergence losses summed over all elements and batch\n",
        " \n",
        "def loss_function(x_hat, x, mu, logvar, step = 1):\n",
        "    \n",
        "    ## Making sure that distributions do not overlap\n",
        "#     loss = nn.functional.binary_cross_entropy(\n",
        "#         x_hat, x, reduction='sum'\n",
        "#     )\n",
        "    #loss = 1.0*MSE(x_hat,x)\n",
        "    #loss = MSE(x_hat,x.view(x.size(0), -1))\n",
        "#     BCE = nn.functional.binary_cross_entropy(\n",
        "#         x_hat, x.view(-1, 256*256*3), reduction='sum'\n",
        "#     )\n",
        "    pix2pix_loss = recon_criterion(x_hat, x)*lambda_recon\n",
        "    ### Makes sure that distributions of each image span entire latent space and the range does not explode\n",
        "    KLD = 0.000005 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2))\n",
        "    if step % 100 == 0:\n",
        "      print(f\"KLD term: {KLD}, pix2pix term: {pix2pix_loss}\")\n",
        "    \n",
        "\n",
        " \n",
        "    return KLD + pix2pix_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18mi63f9B_rg"
      },
      "source": [
        "dataset = []\n",
        "with open(r\"/content/drive/MyDrive/ai4good/train.pkl\", \"rb\") as file:\n",
        "  dataset = pickle.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi33-nt3B9N_"
      },
      "source": [
        "MEAN = (0.5, 0.5, 0.5,)\n",
        "STD = (0.5, 0.5, 0.5,)\n",
        "SIZE = 256\n",
        "\n",
        "class Transform():\n",
        "    def __init__(self, mean=MEAN, std=STD):\n",
        "        self.data_transform = transforms.Compose([ \n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "        \n",
        "    def __call__(self, img: Image.Image):\n",
        "        return self.data_transform(img)\n",
        "\n",
        "class Mask_Transform():\n",
        "    def __init__(self):\n",
        "        self.data_transform = transforms.Compose([ \n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        \n",
        "    def __call__(self, img: Image.Image):\n",
        "        return self.data_transform(img)\n",
        "\n",
        "    \n",
        "class Dataset(object):\n",
        "    def __init__(self, data, aug):\n",
        "        self.data = data \n",
        "        self.transformer = Transform()\n",
        "        self.mask_transform = Mask_Transform()\n",
        "        self.aug = aug\n",
        "        \n",
        "    \n",
        "    def __getitem__(self, idx: int):\n",
        "        image, mask = self.data[idx]\n",
        "        t = self.aug(image = image, mask = mask)\n",
        "        image = t['image']\n",
        "        mask = t['mask']\n",
        "        image = self.transformer(image)\n",
        "        mask = self.mask_transform(mask)\n",
        "        return image, mask\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcqI7dT5CBjH"
      },
      "source": [
        "train_transforms = A.Compose([# D4 Group augmentations\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.VerticalFlip(p=0.5),\n",
        "                A.RandomRotate90(p=0.5),\n",
        "                #A.RandomBrightness(limit = 0.1, p = 0.5),                 \n",
        "                A.Normalize(mean = MEAN, std = STD)\n",
        "                ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzvNLfFiCC8-"
      },
      "source": [
        "#use this to load images per batch slow and sometimes runs a pytorch error\n",
        "#still investigating\n",
        "#data_read = list(zip(img_paths, mask_paths))\n",
        "#train_read = Dataset_read(data_read)\n",
        "train_ds = Dataset(dataset, aug = train_transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa6GzIPzCENH"
      },
      "source": [
        "len(train_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-t9k_8YEAv6"
      },
      "source": [
        "epochs = 10\n",
        "codes = dict(μ=list(), logσ2=list(), y=list())\n",
        "cur_step = 0\n",
        "display_step = 30\n",
        "input_dim = 3\n",
        "out_dim = 1\n",
        "target_shape = 256\n",
        "batch_size = 16\n",
        "dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "loss = {\n",
        "    'train_loss':[],\n",
        "    'test_loss' : []\n",
        "}\n",
        "for epoch in range(0, epochs + 1):\n",
        "    #safety save\n",
        "    torch.save(model.state_dict(), f'/content/drive/MyDrive/vae_{epoch}.pth')\n",
        "    # Training\n",
        "    if epoch > 0:  # test untrained net first\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for image, mask in dataloader:\n",
        "            #x = x.to(device)\n",
        "            # ===================forward=====================\n",
        "            fake_map, mu, logvar, z = model(image)\n",
        "            loss = loss_function(fake_map, mask, mu, logvar, cur_step)\n",
        "            train_loss += loss.item()\n",
        "            # ===================backward====================\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        # ===================log========================\n",
        "        ### Visualization code ###\n",
        "            if cur_step % display_step == 0:\n",
        "                if cur_step > 0:\n",
        "                    print(f\"Epoch {epoch}: Step {cur_step}: Loss = {train_loss / cur_step}\")\n",
        "                show_tensor_images(image, size=(input_dim, target_shape, target_shape))\n",
        "                show_tensor_images(mask, size=(out_dim, target_shape, target_shape))\n",
        "                show_tensor_images(fake_map, size=(out_dim, target_shape, target_shape))\n",
        "                mean_generator_loss = 0\n",
        "                mean_discriminator_loss = 0\n",
        "            cur_step += 1\n",
        "        \n",
        "        #loss['train_loss'].append(train_loss)\n",
        "        \n",
        "    \n",
        "    # Testing\n",
        "    \n",
        "    means, logvars, labels = list(), list(), list()\n",
        "    \n",
        "    '''\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        for x, _ in test_loader:\n",
        "            #x = x.to(device)\n",
        "            # ===================forward=====================\n",
        "            x_hat, mu, logvar,_ = model(x)\n",
        "            test_loss += loss_function(x_hat, x, mu, logvar).item()\n",
        "            # =====================log=======================\n",
        "            means.append(mu.detach())\n",
        "            logvars.append(logvar.detach())\n",
        "            #labels.append(y.detach())\n",
        "    # ===================log========================\n",
        "    #loss['test_loss'].append(test_loss)\n",
        "    codes['μ'].append(torch.cat(means))\n",
        "    codes['logσ2'].append(torch.cat(logvars))\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    if epoch % 1 == 0:\n",
        "        print(f'====> Test set loss: {test_loss:.4f}')\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVyQWlZrFbpN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}